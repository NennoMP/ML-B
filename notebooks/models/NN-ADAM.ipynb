{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d3416e-3aee-496b-9a46-d86fab3fe008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716b3a4-870e-4da1-af2b-323ce7cfb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal, HeNormal, GlorotUniform, Constant, Zeros\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('ML-B')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "from data.data_config import Dataset\n",
    "from data.data_utils import load_monk, load_cup, store_monk_result, store_cup_result\n",
    "from hyperparameter_tuning import grid_search, random_search, tuning_search_top_configs\n",
    "from training.solver import Solver\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c866a15-7062-4d33-a206-2ec35f8643f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Networks\n",
    "In this notebook we implement and test a custom (feed-forward) Neural Network w.r.t. the tasks at hand, i.e. the three MONK's problem and the CUP dataset.\n",
    "\n",
    "The employed optimizer is **ADAM with mini-batch**.\n",
    "\n",
    "\n",
    "Specifically:\n",
    "- **get_nn_classifier(...)**: defines the (ADAM) NN classifier for the MONK's problems;\n",
    "- **get_nn_adam_regressor(...)**: defines the (ADAM) NN regressor for the CUP dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a076d-71ee-46dc-949f-caa3a3bf2df7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfe3c3-e764-453e-879e-96937623ab48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'NN-ADAM'\n",
    "INTERNAL_TEST_SPLIT = 0.1 # internal test split percentage\n",
    "RANDOM_STATE = 128 # reproducibility\n",
    "N_SPLITS = 5 # cross-validation\n",
    "POLY_DEGREE = 3 # polynomial features pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84564330-d564-4700-9865-88576073b71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b342ae0-1ef2-4bdb-9128-cd10ac83adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "results_dir = root_path + '/results/' + MODEL_NAME\n",
    "\n",
    "# Filepaths (MONK)\n",
    "m1_dev_path, m1_test_path = Dataset.MONK_1.dev_path, Dataset.MONK_1.test_path # MONK 1\n",
    "m2_dev_path, m2_test_path = Dataset.MONK_2.dev_path, Dataset.MONK_2.test_path # MONK 2\n",
    "m3_dev_path, m3_test_path = Dataset.MONK_3.dev_path, Dataset.MONK_3.test_path # MONK 3\n",
    "\n",
    "# Filepaths (CUP)\n",
    "cup_dev_path, cup_test_path = Dataset.CUP.dev_path, Dataset.CUP.test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859824b9-6c6c-4f09-b4d9-ad5f9c5e2b29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MONK-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cca8a-3ad3-4f0f-a1d8-8ff6253f962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MONK-1\n",
    "x_dev_m1, y_dev_m1, x_test_m1, y_test_m1 = load_monk(m1_dev_path, m1_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad13309-e6ed-43ce-b6be-b641c3a9f418",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787a40f-87d1-45e9-b24d-90b45367ab60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nn_classifier(hparams):\n",
    "    \"\"\"Returns a NN with ADAM classifier.\n",
    "    \n",
    "    Args:\n",
    "        - hparams: a set of hyper-parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    if hparams['activation'] == 'tanh':\n",
    "        initializer = GlorotUniform(seed=RANDOM_STATE) # Glorot (Xavier)\n",
    "    elif hparams['activation'] == 'ReLU':\n",
    "        initializer = HeNormal(seed=RANDOM_STATE) # He (Kaiming)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(\n",
    "            hparams['h_dim'], \n",
    "            activation=hparams['activation'], \n",
    "            input_shape=(17,), \n",
    "            kernel_initializer=initializer),\n",
    "        Dense(1, activation='sigmoid', kernel_regularizer=l2(hparams['reg']))\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=hparams['lr'], beta_1=hparams['beta_1'], beta_2=hparams['beta_2'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    model.hparams = hparams\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5a366-1520-4768-a26e-f461426e2393",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156513a3-6ad0-40df-99ff-12670c23168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m1 = get_nn_classifier(hparams={'lr': 0.1, 'h_dim': 4, 'activation': 'ReLU', 'reg': 0, 'beta_1': 0.9, 'beta_2': 0.99})\n",
    "solver = Solver(model_m1, x_dev_m1, y_dev_m1, x_test_m1, y_test_m1, target='accuracy')\n",
    "solver.train(epochs=400, patience=None, batch_size=len(x_dev_m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b222ca-ff80-475e-b63e-85b4ebdc4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.plot_history(results_dir + '/MONK1/history', 'MSE', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac744c-68b2-412d-ae72-cc3126fd83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- DEVELOPMENT --')\n",
    "mse_dev_m1, acc_dev_m1 = model_m1.evaluate(x_dev_m1, y_dev_m1)\n",
    "print(f'Accuracy: {acc_dev_m1:.4f} - MSE: {mse_dev_m1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e328d0-7a8e-463d-b9f3-0b2bdd3700c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- TEST --')\n",
    "mse_test_m1, acc_test_m1 = model_m1.evaluate(x_test_m1, y_test_m1)\n",
    "print(f'Accuracy: {acc_test_m1:.4f} - MSE: {mse_test_m1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a9e02-7de5-4fe0-8b53-5d7e98839b89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b49cf-3f01-496d-a99a-d7bded86f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_m1 = {\n",
    "    'dev': {'accuracy': acc_dev_m1, 'mse': mse_dev_m1},\n",
    "    'test': {'accuracy': acc_test_m1, 'mse': mse_test_m1}\n",
    "}\n",
    "\n",
    "store_monk_result(results_dir + '/MONK1/', model_m1.hparams, report_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216aed0-5ba0-4816-a889-a3be455e73c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MONK-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efe875-d6ff-42c0-b719-85a47a5855b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MONK-2\n",
    "x_dev_m2, y_dev_m2, x_test_m2, y_test_m2 = load_monk(m2_dev_path, m2_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f49c0-396e-4904-909a-7b2c01076bad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37344b75-75aa-413a-8544-48beb5591d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m2 = get_nn_classifier(hparams={'lr': 0.09, 'h_dim': 4, 'activation': 'ReLU', 'reg': 0, 'beta_1': 0.9, 'beta_2': 0.99})\n",
    "solver = Solver(model_m2, x_dev_m2, y_dev_m2, x_test_m2, y_test_m2, target='val_accuracy')\n",
    "solver.train(epochs=400, patience=None, batch_size=len(x_dev_m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1aee3-c232-4f80-a3eb-c363aa7de7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.plot_history(results_dir + '/MONK2/history', 'MSE', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52af39-5893-4dcf-bdf2-8703c6e202fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- DEVELOPMENT --')\n",
    "mse_dev_m2, acc_dev_m2 = model_m2.evaluate(x_dev_m2, y_dev_m2)\n",
    "print(f'Accuracy: {acc_dev_m2:.4f} - MSE: {mse_dev_m2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc4807-03c1-421c-bc8a-a7b2154e6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- TEST --')\n",
    "mse_test_m2, acc_test_m2 = model_m2.evaluate(x_test_m2, y_test_m2)\n",
    "print(f'Accuracy: {acc_test_m2:.4f} - MSE: {mse_test_m2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ca457-cb8a-48d7-9c9e-4e9d473efe3a",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ead2f-5368-4648-b36c-73f9aa69b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_m2 = {\n",
    "    'dev': {'accuracy': acc_dev_m2, 'mse': mse_dev_m2},\n",
    "    'test': {'accuracy': acc_test_m2, 'mse': mse_test_m2}\n",
    "}\n",
    "\n",
    "store_monk_result(results_dir + '/MONK2/', model_m2.hparams, report_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc48b71-c462-4d78-a2cd-f3a4102e5b4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MONK-3 (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51bbd4-fdca-48cc-8c47-17eb4254af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MONK-3\n",
    "x_dev_m3, y_dev_m3, x_test_m3, y_test_m3 = load_monk(m3_dev_path, m3_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e7196-0baf-459c-8f8b-df9119d32eb6",
   "metadata": {},
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57949d3-b8f0-4717-a28d-9e527a0ad986",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m3 = get_nn_classifier(hparams={'lr': 0.01, 'h_dim': 4, 'reg': 0, 'activation': 'tanh', 'beta_1': 0.9, 'beta_2': 0.9})\n",
    "solver = Solver(model_m3, x_dev_m3, y_dev_m3, x_test_m3, y_test_m3, target='accuracy')\n",
    "solver.train(epochs=400, patience=None, batch_size=len(x_dev_m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28098b-fe70-4b75-9142-f2e2c8e74404",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.plot_history(results_dir + '/MONK3/history', 'MSE', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733829b-b685-474e-b51b-fa653c757c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- DEVELOPMENT --')\n",
    "mse_dev_m3, acc_dev_m3 = model_m3.evaluate(x_dev_m3, y_dev_m3)\n",
    "print(f'Accuracy: {acc_dev_m3:.4f} - MSE: {mse_dev_m3:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628cf725-1bc8-4e20-949c-502c7863a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- TEST --')\n",
    "mse_test_m3, acc_test_m3 = model_m3.evaluate(x_test_m3, y_test_m3)\n",
    "print(f'Accuracy: {acc_test_m3:.4f} - MSE: {mse_test_m3:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf13d3-919d-400d-ba43-f5a3f9f2d5f6",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed086e45-cc54-4581-a75a-c27ee7720155",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_m3 = {\n",
    "    'dev': {'accuracy': acc_dev_m3, 'mse': mse_dev_m3},\n",
    "    'test': {'accuracy': acc_test_m3, 'mse': mse_test_m3}\n",
    "}\n",
    "\n",
    "store_monk_result(results_dir + '/MONK3/', model_m3.hparams, report_m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2e408-6d1b-4c43-b70b-c4fec0da831d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MONK-3 (with regularization)\n",
    "MONK3 when setting some regularization value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef02d30-2efb-4f50-9f4c-c097cb021264",
   "metadata": {},
   "source": [
    "## Training - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd530d6-28ff-4dca-8616-33a16fae5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m3_reg = get_nn_classifier(hparams={'lr': 0.01, 'h_dim': 4, 'reg': 0.01, 'activation': 'tanh', 'beta_1': 0.9, 'beta_2': 0.9})\n",
    "solver = Solver(model_m3_reg, x_dev_m3, y_dev_m3, x_test_m3, y_test_m3, target='accuracy')\n",
    "solver.train(epochs=400, patience=None, batch_size=len(x_dev_m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b597b3-6134-4456-8440-c463ef84eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.plot_history(results_dir + '/MONK3-reg/history', 'MSE', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89050a9-a2d0-4a50-9ba1-8839be3bc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- DEVELOPMENT --')\n",
    "mse_dev_m3_reg, acc_dev_m3_reg = model_m3_reg.evaluate(x_dev_m3, y_dev_m3)\n",
    "print(f'Accuracy: {acc_dev_m3_reg:.4f} - MSE: {mse_dev_m3_reg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7771d-67c6-433f-b0ef-da8f41c7fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- TEST --')\n",
    "mse_test_m3_reg, acc_test_m3_reg = model_m3_reg.evaluate(x_test_m3, y_test_m3)\n",
    "print(f'Accuracy: {acc_test_m3_reg:.4f} - MSE: {mse_test_m3_reg:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb110c7d-e148-4c70-9be3-e43803d20f10",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77e7d4-97a9-4ceb-87d0-a4a2a0f0f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_m3_reg = {\n",
    "    'dev': {'accuracy': acc_dev_m3_reg, 'mse': mse_dev_m3_reg},\n",
    "    'test': {'accuracy': acc_test_m3_reg, 'mse': mse_test_m3_reg}\n",
    "}\n",
    "\n",
    "store_monk_result(results_dir + '/MONK3-reg/', model_m3_reg.hparams, report_m3_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f6603-97eb-4bf3-ae71-b1a62be020bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7e3ff-908a-480a-bc16-bd7a020f41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CUP\n",
    "x_dev_cup, y_dev_cup, x_test_cup = load_cup(cup_dev_path, cup_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274b55b-f97f-48d0-8d4c-8afa1c001b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "def mean_euclidean_error(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Utility function to compute the Mean Euclidean Error (MEE) between \n",
    "    true and predicted values for a tensorflow model. \n",
    "    Return the MEE score as a tensor.\n",
    "\n",
    "    Required arguments:\n",
    "    - y_true: array containing true values (ground truth).\n",
    "    - y_pred: array containing predicted values.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_pred - y_true), axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ae8aa-990f-4b6d-a301-186c83d60e1e",
   "metadata": {},
   "source": [
    "## Dev - Internal Test Split \n",
    "The development dataset is split between training and internal test ($90-10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a0d14-f694-4424-a2f8-f497f42203e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dev data into train - internal test\n",
    "x_train_cup, x_internal_test_cup, y_train_cup, y_internal_test_cup = train_test_split(\n",
    "    x_dev_cup, \n",
    "    y_dev_cup, \n",
    "    test_size=INTERNAL_TEST_SPLIT, \n",
    "    random_state=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de9dfd-8491-48f1-a56d-927498a0e0eb",
   "metadata": {},
   "source": [
    "## Polynomial features pre-processing\n",
    "We create a version of our dataset to which PolynoMialFeatures pre-processing is applied with a fixed degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2280b-3db2-466e-a970-f89845e43551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMMENT TO USE NON-ENCHANED DATASET --- \n",
    "# Polynomial features pre-processing\n",
    "poly = PolynomialFeatures(degree=POLY_DEGREE)\n",
    "x_train_cup = poly.fit_transform(x_train_cup)\n",
    "x_internal_test_cup = poly.transform(x_internal_test_cup)\n",
    "x_test_cup = poly.transform(x_test_cup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d14979-19ac-4d82-86a2-d7c8e2aedf9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NN-Adam (mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02535f-4286-4ebb-93d3-02e32e1c5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_adam_regressor(hparams: dict, in_dim: int):\n",
    "    \"\"\"Returns a NN with ADAM regressor.\n",
    "    \n",
    "    Args:\n",
    "        - hparams: a set of hyper-parameters\n",
    "        - in_dim: input dimension [1]\n",
    "    \"\"\"\n",
    "    \n",
    "    if hparams['activation'] == 'tanh':\n",
    "        initializer = GlorotUniform(seed=RANDOM_STATE) # Glorot (Xavier)\n",
    "        bias_initializer = Zeros()\n",
    "    elif hparams['activation'] == 'ReLU':\n",
    "        initializer = HeNormal(seed=RANDOM_STATE) # He (Kaiming)\n",
    "        bias_initializer = Constant(0.1)\n",
    "        \n",
    "    reg = l2(hparams['reg'])\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        hparams['h_dim'], \n",
    "        activation=hparams['activation'], \n",
    "        input_shape=(in_dim,), \n",
    "        kernel_regularizer=l2(hparams['reg']),\n",
    "        kernel_initializer=initializer,\n",
    "        bias_initializer=bias_initializer))\n",
    "\n",
    "\n",
    "    h_dim = hparams['h_dim']\n",
    "    for i in range(hparams['n_layers'] - 1):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                h_dim, \n",
    "                activation=hparams['activation'],\n",
    "                kernel_regularizer=l2(hparams['reg']),\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=bias_initializer))\n",
    "        h_dim //= 2\n",
    "\n",
    "\n",
    "    model.add(Dense(\n",
    "        3, \n",
    "        activation='linear', \n",
    "        kernel_regularizer=l2(hparams['reg']), \n",
    "        kernel_initializer=initializer,\n",
    "        bias_initializer=bias_initializer))\n",
    "    \n",
    "    optimizer = Adam(\n",
    "        learning_rate=hparams['lr'],\n",
    "        beta_1=hparams['beta_1'], \n",
    "        beta_2=hparams['beta_2'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[mean_euclidean_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a8833-8e33-4b68-938c-3eee6f2d179f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyper-parameters Tuning\n",
    "A common approach is to start with a coarse search across a wide range of values to find promising sub-ranges of our parameter space. Then, you would zoom into these ranges and perform another search to fine-tune the configurations.\n",
    "\n",
    "Here, we proceed as follows:\n",
    "1. (coarse) Grid-search across a wide range of hyper-paramaters and values;\n",
    "2. (fine-tune) Random-search into zoomed intervals w.r.t. best configuration found by grid-search.\n",
    "\n",
    "Then, we perform a single run of grid-search and random-search with the respectively best configurations while taking into account a PolynomialFeatures pre-processing with fixed degree. The best configurations that will be used for final re-training and evaluation on internal test is the one with the best mean MEE on the validation cross-validation.\n",
    "\n",
    "Note that, tuning of the polynomial degree wasn't performed because it would be very expensive. Thus, we simply decided to use a fixed degree value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc749ab-1dde-4023-8369-0653c7f695fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61da62-27b3-4a5e-9739-892cf8bd4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_spaces_cup = {\n",
    "    'lr': [0.0001, 0.0005, 0.001],\n",
    "    'n_layers': [2, 3],\n",
    "    'h_dim': [32, 64, 128],\n",
    "    'activation': ['tanh'],\n",
    "    'reg': [0.01, 0.001],\n",
    "    'beta_1': [0.8, 0.9],\n",
    "    'beta_2': [0.9, 0.95, 0.99],\n",
    "    'batch_size': [32, 64, 128],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366388a7-721c-42a5-aed9-d6121f075e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search (coarse)\n",
    "best_model_coarse, best_config_coarse, results_coarse = grid_search(\n",
    "    get_nn_adam_regressor,\n",
    "    x_train_cup.shape[1],\n",
    "    x_train_cup,\n",
    "    y_train_cup,\n",
    "    grid_search_spaces_cup, \n",
    "    target='val_mean_euclidean_error', \n",
    "    N_SPLITS=N_SPLITS,\n",
    "    EPOCHS=800, \n",
    "    PATIENCE=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815dd66-dba9-404a-a400-0509096bb571",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1cf37-0020-40e4-ae34-803e74a2c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new intervals for fine-tune random search\n",
    "lr = best_config_coarse['lr']\n",
    "n_layers = best_config_coarse['n_layers']\n",
    "h_dim = best_config_coarse['h_dim']\n",
    "activation = best_config_coarse['activation']\n",
    "reg = best_config_coarse['reg']\n",
    "beta_1 = best_config_coarse['beta_1']\n",
    "beta_2 = best_config_coarse['beta_2']\n",
    "batch_size = best_config_coarse['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5093f-d692-4bcd-bf5d-02869f261ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "random_search_spaces_cup = {\n",
    "    'lr': ([10 ** (np.log10(lr) - epsilon), 10 ** (np.log10(lr) + epsilon)], 'float'),\n",
    "    'n_layers': ([n_layers], 'item'),\n",
    "    'h_dim': ([h_dim - 20, h_dim + 20], 'item'),\n",
    "    'activation': ([activation], 'item'),\n",
    "    'reg': ([10 ** (np.log10(reg) - epsilon), 10 ** (np.log10(reg) + epsilon)], 'float'),\n",
    "    'beta_1': ([10 ** (np.log10(beta_1) - epsilon), 10 ** (np.log10(beta_1) + epsilon)], 'float'),\n",
    "    'beta_2': ([10 ** (np.log10(beta_2) - epsilon), 10 ** (np.log10(beta_2) + epsilon)], 'float'),\n",
    "    'batch_size': ([batch_size], 'item'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f8061-fc4a-42b7-863b-3080e2a82009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search (fine-tune)\n",
    "best_model_finetune, best_config_finetune, results_finetune = random_search(\n",
    "    get_nn_adam_regressor, \n",
    "    x_train_cup.shape[1],\n",
    "    x_train_cup,\n",
    "    y_train_cup,\n",
    "    random_search_spaces_cup, \n",
    "    target='val_mean_euclidean_error', \n",
    "    N_SPLITS=N_SPLITS,\n",
    "    NUM_SEARCH=20,\n",
    "    EPOCHS=800, \n",
    "    PATIENCE=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a62d4-1ac7-421c-844d-81db28b83ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ec3f7-d0f7-45e9-894d-32a8d5a08648",
   "metadata": {},
   "outputs": [],
   "source": [
    "mee_coarse = best_config_coarse['val_mean_euclidean_error']\n",
    "mee_finetune = best_config_finetune['val_mean_euclidean_error']\n",
    "\n",
    "best_config_cup = best_config_finetune if mee_finetune < mee_coarse else best_config_coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1a54e-8ba2-46b0-ae8d-dfd92473f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store grid-search\n",
    "best_config_coarse['n_layers'] = int(best_config_coarse['n_layers'])\n",
    "best_config_coarse['h_dim'] = int(best_config_coarse['h_dim'])\n",
    "best_config_coarse['batch_size'] = int(best_config_coarse['batch_size'])\n",
    "with open(results_dir + '/CUP/grid_search.json', 'w') as outf:\n",
    "    json.dump(best_config_coarse, outf, indent=4)\n",
    "    \n",
    "# Store random-search\n",
    "best_config_finetune['n_layers'] = int(best_config_finetune['n_layers'])\n",
    "best_config_finetune['h_dim'] = int(best_config_finetune['h_dim'])\n",
    "best_config_finetune['batch_size'] = int(best_config_finetune['batch_size'])\n",
    "with open(results_dir + '/CUP/random_search.json', 'w') as outf:\n",
    "    json.dump(best_config_finetune, outf, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f830e9-1e7e-4d77-80d8-1a73a6ca3eed",
   "metadata": {},
   "source": [
    "## Training and Internal test assessment\n",
    "Let's perform a re-training of our model on the entire training/validation set. In this way, we're able to leverage the entire training/validation data (early stopping is applied w.r.t. the train MEE). Finally, predict on the (untouched) internal test to perform model assessment and estimate our performance on the blind test set.\n",
    "\n",
    "Note that, in this phase, we don't use the internal test in any way (i.e., no training and no validation). We only estimate its errors and plot its curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f520c8-2397-44c4-a23e-884fc11f6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UNCOMMENT TO TEST ---\n",
    "# Best (tuning) configuration\n",
    "# Best configuration\n",
    "best_config_cup = {\n",
    "    'lr': 0.00026, \n",
    "    'n_layers': 3, \n",
    "    'h_dim': 128, \n",
    "    'activation': 'tanh', \n",
    "    'reg': 0.001, \n",
    "    'beta_1': 0.87, \n",
    "    'beta_2': 0.96, \n",
    "    'batch_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e402234-469d-4e6e-a2d9-c73f45756159",
   "metadata": {},
   "source": [
    "We define a warmup learning-rate scheduler (higher learning rate to lower), since we have noticed that the Neural Network with ADAM optimizer has a particularly slow convergence, mostly due to very high values of MSE (loss) in the first epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdda99-85f8-471c-8155-56f3faac6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr(epoch, lr):\n",
    "    \"\"\"Defines a warm-up learning rate approach to speed-up convergence.\"\"\"\n",
    "    \n",
    "    warmup_epochs = 10  # number of epochs for the warm-up\n",
    "    base_lr = best_config_cup['lr'] # desired learning rate\n",
    "    initial_lr = 0.01  # starting learning rate\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linearly decrease learning rate\n",
    "        lr = initial_lr + (base_lr - initial_lr) * epoch / warmup_epochs\n",
    "    else:\n",
    "        # keep the learning rate constant after warm-up\n",
    "        lr = base_lr\n",
    "\n",
    "    return lr\n",
    "    \n",
    "lr_scheduler = LearningRateScheduler(warmup_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a131f-64ea-42ad-bf40-27750acb17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train to plot learning curves\n",
    "model_nn_cup = get_nn_adam_regressor(best_config_cup, x_train_cup.shape[1])\n",
    "solver = Solver(model_nn_cup, x_train_cup, y_train_cup, x_internal_test_cup, y_internal_test_cup, target='mean_euclidean_error')\n",
    "solver.train(epochs=1500, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400b3ff-c8be-470c-917f-1210fb910db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store learning curves\n",
    "solver.plot_history(results_dir + '/CUP/history_poly', loss='MSE', metric='MEE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b4f42-c615-4575-b25d-ee25c50670ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training average\n",
    "To obtain a more accurate estimate on the internal test (and thus on the blind test), we perform 5 trials with the exact same setting, and then take the average.\n",
    "In this phase, we don't use the internal test in any way (i.e., no training and no validation). We only predict \n",
    "\n",
    "Specifically, we train execute $<N\\_TRIALS>$ trials of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad141ed3-f8a8-405a-908c-c148bcf0910b",
   "metadata": {},
   "source": [
    "To obtain a more accurate estimate on the internal test (and thus on the blind test), we perform 5 trials with the exact same setting, and then take the average.\n",
    "\n",
    "Specifically, we train execute $<N\\_TRIALS>$ trials of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3c38c-b7e7-46fe-94bd-530b9d89a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 5\n",
    "\n",
    "# Keep track of the trials results\n",
    "train_preds_arr = np.zeros((N_TRIALS, 900, 3))\n",
    "internal_test_preds_arr = np.zeros((N_TRIALS, 100, 3))\n",
    "test_preds_arr = np.zeros((N_TRIALS, 900, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd050d8-348b-4064-ba1c-3d8807f4ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5 training trials\n",
    "for i in range(N_TRIALS):\n",
    "    # Train \n",
    "    model_nn_cup = get_nn_adam_regressor(best_config_cup, x_train_cup.shape[1])\n",
    "    solver = Solver(model_nn_cup, x_train_cup, y_train_cup, x_internal_test_cup, y_internal_test_cup, target='mean_euclidean_error')\n",
    "    solver.train(epochs=1500, patience=50)\n",
    "\n",
    "    # Store train predictions\n",
    "    train_preds_arr[i] = model_nn_cup.predict(x_train_cup)\n",
    "    # Store internal test predictions\n",
    "    internal_test_preds_arr[i] = model_nn_cup.predict(x_internal_test_cup)\n",
    "    # Blind test predictions\n",
    "    test_preds_arr[i] = model_nn_cup.predict(x_test_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3637236-8be9-484b-9111-5418b7d21881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- TRAINING (MEAN 5 TRIALS) --')\n",
    "loss_train_cup_mean, mee_train_cup_mean = mean_squared_error(y_train_cup, np.mean(train_preds_arr, axis=0)), mean_euclidean_error(y_train_cup, np.mean(train_preds_arr, axis=0))\n",
    "print(f'Mean Loss (MSE): {loss_train_cup_mean:.4f} - Mean MEE: {mee_train_cup_mean:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244ea03-8c41-4b61-b082-9354a62f17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- INTERNAL TEST (MEAN 5 TRIALS) --')\n",
    "loss_internal_test_mean, mee_internal_test_mean = mean_squared_error(y_internal_test_cup, np.mean(internal_test_preds_arr, axis=0)), mean_euclidean_error(y_internal_test_cup, np.mean(internal_test_preds_arr, axis=0))\n",
    "print(f'Mean Loss (MSE): {loss_internal_test_mean:.4f} -  Mean MEE: {mee_internal_test_mean:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d3e34-7012-42bd-974a-0425768cab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blind test set predictions\n",
    "mean_preds_cup = np.mean(test_preds_arr, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f54604-a87f-4a68-88c0-e10425b045a4",
   "metadata": {},
   "source": [
    "### Store result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39bfbc-f96d-4194-afc7-3a13a9848409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store report\n",
    "report_nn_mean = {\n",
    "    'train': {\n",
    "        'mean_mse': loss_train_cup_mean, \n",
    "        'mean_mee': mee_train_cup_mean,\n",
    "    },\n",
    "    'internal_test': {\n",
    "        'mean_mse': loss_internal_test_mean, \n",
    "        'mean_mee': mee_internal_test_mean,\n",
    "    }\n",
    "}\n",
    "\n",
    "store_cup_result(results_dir + '/CUP/mean_5_', best_config_cup, report_nn_mean, mean_preds_cup, is_poly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33afa9-3320-499d-a149-ffeb693c2d02",
   "metadata": {},
   "source": [
    "Store the train and internal test (mean) predictions so that they can be used for the estimate of the ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34eee4-b163-459a-8639-b48aaee2e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train mean predictions\n",
    "with open(results_dir + '/CUP/mean_5_train_preds_poly.csv', 'w') as outf:\n",
    "    # Team Info\n",
    "    outf.write(\"# Matteo Pinna, Leonardo Caridi, Marco Sanna\\n\")\n",
    "    outf.write(\"# ACD-TEAM\\n\")\n",
    "    outf.write(\"# ML-CUP23 v2\\n\")\n",
    "    outf.write(\"# 20/01/2024\\n\")\n",
    "\n",
    "    # Writing predictions\n",
    "    for i, pred in enumerate(np.mean(train_preds_arr, axis=0), 1):\n",
    "        outf.write(f\"{i},{','.join(map(str, pred))}\\n\")\n",
    "        \n",
    "# Store internal test mean predictions\n",
    "with open(results_dir + '/CUP/mean_5_internal_test_preds_poly.csv', 'w') as outf:\n",
    "    # Team Info\n",
    "    outf.write(\"# Matteo Pinna, Leonardo Caridi, Marco Sanna\\n\")\n",
    "    outf.write(\"# ACD-TEAM\\n\")\n",
    "    outf.write(\"# ML-CUP23 v2\\n\")\n",
    "    outf.write(\"# 20/01/2024\\n\")\n",
    "\n",
    "    # Writing predictions\n",
    "    for i, pred in enumerate(np.mean(internal_test_preds_arr, axis=0), 1):\n",
    "        outf.write(f\"{i},{','.join(map(str, pred))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe3ac6-741d-43f4-879d-fcf295945ae0",
   "metadata": {},
   "source": [
    "# Final re-training\n",
    "Since the test error has already been estimated by leveraging the (untouched) internal test, we now perform a final re-training with all the development data. This does not violate the rules, since the internal test is not (and has never) been used for any model selection.\n",
    "\n",
    "Thus, we train on the entire development data, i.e. $90$ train/val + $10$ internal test. Early stopping is w.r.t. the MEE.\n",
    "\n",
    "This final re-training is the one we use to submit the blind test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce27f33-1615-465c-b5b2-a5445e0e26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply polynomial to the entire development set\n",
    "x_dev_cup = poly.transform(x_dev_cup)\n",
    "\n",
    "# Keep track of the trials results\n",
    "dev_preds_arr = np.zeros((N_TRIALS, 1000, 3))\n",
    "final_blind_test_preds_arr = np.zeros((N_TRIALS, 900, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771d58a-b039-4ef4-91c6-bef9b135c15b",
   "metadata": {},
   "source": [
    "This time - since the internal test estimate has already been performed - we're able to use the internal test set either as training data or as validation for the early stopping mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cc345-d457-4863-a164-c4c65e6529be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5 training trials\n",
    "for i in range(N_TRIALS):\n",
    "    # Train (early stopping on validation/internal test MEE)\n",
    "    model_nn_cup = get_nn_adam_regressor(best_config_cup, x_train_cup.shape[1])\n",
    "    solver = Solver(model_nn_cup, x_dev_cup, y_dev_cup, target='mean_euclidean_error')\n",
    "    solver.train(epochs=800, patience=50)\n",
    "\n",
    "    # Dev predictions\n",
    "    dev_preds_arr[i] = model_nn_cup.predict(x_dev_cup)\n",
    "    # Blind test predictions\n",
    "    final_blind_test_preds_arr[i] = model_nn_cup.predict(x_test_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199cf51-16a8-4931-b45d-103bf8d6fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- DEVELOPMENT (MEAN 5 TRIALS) --')\n",
    "final_train_preds = np.mean(dev_preds_arr, axis=0)\n",
    "loss_dev_cup_mean, mee_dev_cup_mean = mean_squared_error(y_dev_cup, final_train_preds), mean_euclidean_error(y_dev_cup, final_train_preds)\n",
    "print(f'Mean Loss (MSE): {loss_dev_cup_mean:.4f} - Mean MEE: {mee_dev_cup_mean:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239a454-9585-4b14-ab53-950e5a607363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final blind test set predictions\n",
    "final_blind_test_preds = np.mean(final_blind_test_preds_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be5473-e73f-4171-b1cc-f5c468ab97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final re-training train/dev mean predictions\n",
    "with open(results_dir + '/CUP/final_train.csv', 'w') as outf:\n",
    "    # Team Info\n",
    "    outf.write(\"# Matteo Pinna, Leonardo Caridi, Marco Sanna\\n\")\n",
    "    outf.write(\"# ACD-TEAM\\n\")\n",
    "    outf.write(\"# ML-CUP23 v2\\n\")\n",
    "    outf.write(\"# 20/01/2024\\n\")\n",
    "\n",
    "    # Writing predictions\n",
    "    for i, pred in enumerate(final_train_preds, 1):\n",
    "        outf.write(f\"{i},{','.join(map(str, pred))}\\n\")\n",
    "\n",
    "# Store final blind test mean predictions\n",
    "with open(results_dir + '/CUP/final_blind_test.csv', 'w') as outf:\n",
    "    # Team Info\n",
    "    outf.write(\"# Matteo Pinna, Leonardo Caridi, Marco Sanna\\n\")\n",
    "    outf.write(\"# ACD-TEAM\\n\")\n",
    "    outf.write(\"# ML-CUP23 v2\\n\")\n",
    "    outf.write(\"# 20/01/2024\\n\")\n",
    "\n",
    "    # Writing predictions\n",
    "    for i, pred in enumerate(final_blind_test_preds, 1):\n",
    "        outf.write(f\"{i},{','.join(map(str, pred))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
