{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d3416e-3aee-496b-9a46-d86fab3fe008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15985a7e-4206-4f13-aaad-d38982df5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, ValidationCurveDisplay\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal, HeNormal, GlorotUniform, Constant, Zeros\n",
    "from keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('ML-B')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "from data.data_config import Dataset\n",
    "from data.data_utils import load_monk, load_cup, store_monk_result, store_cup_result\n",
    "from hyperparameter_tuning import grid_search, random_search, tuning_search_top_configs\n",
    "from training.solver import Solver\n",
    "from training.metrics import mean_euclidean_error as mee\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c866a15-7062-4d33-a206-2ec35f8643f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ensemble\n",
    "In this notebook we apply several ensemble techniques to combine the blind test predictions of our (best) chosen model.\n",
    "\n",
    "Specifically, the ensemble approaches we consider are the following:\n",
    "- *Arithmetic Average*;\n",
    "- *Weighted Average*;\n",
    "- *Stacking*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a076d-71ee-46dc-949f-caa3a3bf2df7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfe3c3-e764-453e-879e-96937623ab48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Ensemble'\n",
    "VAL_SPLIT = 0.2 # validation split percentage\n",
    "INTERNAL_TEST_SPLIT = 0.1 # internal test split percentage\n",
    "RANDOM_STATE = 128 # reproducibility\n",
    "N_SPLITS = 5 # cross-validation\n",
    "POLY_DEGREE = 3 # Polynomial features pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84564330-d564-4700-9865-88576073b71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b342ae0-1ef2-4bdb-9128-cd10ac83adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "results_dir = root_path + '/results/' + MODEL_NAME\n",
    "\n",
    "# Filepaths (MONK)\n",
    "m1_dev_path, m1_test_path = Dataset.MONK_1.dev_path, Dataset.MONK_1.test_path # MONK 1\n",
    "m2_dev_path, m2_test_path = Dataset.MONK_2.dev_path, Dataset.MONK_2.test_path # MONK 2\n",
    "m3_dev_path, m3_test_path = Dataset.MONK_3.dev_path, Dataset.MONK_3.test_path # MONK 3\n",
    "\n",
    "# Filepaths (CUP)\n",
    "cup_dev_path, cup_test_path = Dataset.CUP.dev_path, Dataset.CUP.test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670a929-fe09-4c03-8b5c-0869269b335c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Arithmetic Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b624b-0408-41f4-b380-88aea7bdf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_arithmetic_average(nn_sgd_preds, nn_adam_preds, svr_preds):\n",
    "    \"\"\"Compute and return the arithmetic average of three models predictions.\"\"\"\n",
    "    #return (svr_preds + nn_sgd_preds + nn_adam_preds) / 3\n",
    "    return np.mean(np.array([nn_adam_preds, nn_sgd_preds, svr_preds]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440de32b-5022-4775-aa7e-dee17990b14d",
   "metadata": {},
   "source": [
    "## Weighted Averange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af137b-9ef8-411d-9f3c-ccf571869792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weight used is (N - mee)\n",
    "def ensemble_weighted_averange(N: float, svr_preds, nn_sgd_preds, nn_adam_preds, mee_svr: float, mee_nn_sgd: float, mee_nn_adam: float):\n",
    "    \"\"\"Compute and return the weighted average of the three models predictions.\n",
    "    \n",
    "    Args:\n",
    "        - N: the maximum (worse) value possible\n",
    "    \"\"\"\n",
    "    \n",
    "    if (N < mee_svr or N < mee_nn_sgd or N < mee_nn_adam):\n",
    "        raise ValueError(\"N must be greater than mee\")\n",
    "        \n",
    "    w_nom = svr_preds*(N-mee_svr)**2 + nn_sgd_preds*(N-mee_nn_sgd)**2 + nn_adam_preds*(N-mee_nn_adam)**2\n",
    "    w_den = (N - mee_svr)**2 + (N - mee_nn_sgd)**2 + (N - mee_nn_adam)**2\n",
    "            \n",
    "    return w_nom / w_den"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f6603-97eb-4bf3-ae71-b1a62be020bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CUP\n",
    "According to the mean validation MEE achieved during KFold cross-validation, we have selected three models for our ensembles.\n",
    "\n",
    "The chosen models are:\n",
    "- *Neural Network with SGD and mini-batch (NN-SGD)*;\n",
    "- *Neural Network with ADAM and mini-batch (NN-ADAM)*;\n",
    "- Support Vector Regressor (SVR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7e3ff-908a-480a-bc16-bd7a020f41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CUP\n",
    "x_dev_cup, y_dev_cup, x_test_cup = load_cup(cup_dev_path, cup_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274b55b-f97f-48d0-8d4c-8afa1c001b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "def mean_euclidean_error(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Utility function to compute the Mean Euclidean Error (MEE) between \n",
    "    true and predicted values for a tensorflow model. \n",
    "    Return the MEE score as a tensor.\n",
    "\n",
    "    Required arguments:\n",
    "    - y_true: array containing true values (ground truth).\n",
    "    - y_pred: array containing predicted values.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_pred - y_true), axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ae8aa-990f-4b6d-a301-186c83d60e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dev - Internal Test Split \n",
    "The development dataset is split between training and internal test ($90-10$). Then, to decide whether to take a specific model or ensemble, we further split the train data into train and validation. Thus, the final split of the development data is ($70-20-10$) for training, validation and internal set respectively.\n",
    "\n",
    "The validation MEE will be used for our final model choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83289b33-c939-44ea-8a5f-556038dab721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dev data into train - internal test\n",
    "x_dev_cup_tmp, x_internal_test_cup, y_dev_cup_tmp, y_internal_test_cup = train_test_split(\n",
    "    x_dev_cup, \n",
    "    y_dev_cup, \n",
    "    test_size=INTERNAL_TEST_SPLIT, \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b5a9d-4ac7-4738-b8ee-5e2d446b08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dev data into train - internal test\n",
    "x_train_cup, x_val_cup, y_train_cup, y_val_cup = train_test_split(\n",
    "    x_dev_cup_tmp, \n",
    "    y_dev_cup_tmp, \n",
    "    test_size=(VAL_SPLIT / (1 - INTERNAL_TEST_SPLIT)), \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d1bdf-28f4-442a-8e27-7087dce760ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Polynomial features pre-processing\n",
    "We create a version of our dataset to which PolynoMialFeatures pre-processing is applied with a fixed degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdecf5e5-fbb6-4af5-8960-454c8c649435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features pre-processing\n",
    "poly = PolynomialFeatures(degree=POLY_DEGREE)\n",
    "x_train_cup_poly = poly.fit_transform(x_train_cup)\n",
    "x_val_cup_poly = poly.transform(x_val_cup)\n",
    "x_internal_test_cup_poly = poly.transform(x_internal_test_cup)\n",
    "#x_test_cup = poly.transform(x_test_cup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395c3c2-f7ce-4310-a624-e1e81d63d294",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models' utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911042f0-3a94-4f59-825a-25312f6d0b70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NN-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d01a87-13f5-4927-8c34-7a1316656447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration NN-SGD\n",
    "best_config_sgd = {\n",
    "    'lr': 0.00035,\n",
    "    'h_dim': 100,\n",
    "    'n_layers': 3,\n",
    "    'activation': 'tanh',\n",
    "    'reg': 0.001,\n",
    "    'momentum': 0.95, \n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "def get_nn_sgd_regressor(hparams: dict, in_dim: int):\n",
    "    \"\"\"Returns a NN with SGD regressor.\n",
    "    \n",
    "    Args:\n",
    "        - hparams: a set of hyper-parameters\n",
    "        - in_dim: input dimension [1]\n",
    "    \"\"\"\n",
    "    \n",
    "    if hparams['activation'] == 'tanh':\n",
    "        initializer = GlorotUniform(seed=RANDOM_STATE) # Glorot (Xavier)\n",
    "        bias_initializer = Zeros()\n",
    "    elif hparams['activation'] == 'ReLU':\n",
    "        initializer = HeNormal(seed=RANDOM_STATE) # He (Kaiming)\n",
    "        bias_initializer = Constant(0.1)\n",
    "        \n",
    "    reg = l2(hparams['reg'])\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        hparams['h_dim'], \n",
    "        activation=hparams['activation'], \n",
    "        input_shape=(in_dim,), \n",
    "        kernel_regularizer=l2(hparams['reg']),\n",
    "        kernel_initializer=initializer,\n",
    "        bias_initializer=bias_initializer))\n",
    "\n",
    "    h_dim = hparams['h_dim']\n",
    "    for i in range(hparams['n_layers'] - 1):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                h_dim, \n",
    "                activation=hparams['activation'],\n",
    "                kernel_regularizer=l2(hparams['reg']),\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=bias_initializer))\n",
    "        h_dim //= 2\n",
    "        \n",
    "    model.add(Dense(\n",
    "        3, \n",
    "        activation='linear', \n",
    "        kernel_regularizer=l2(hparams['reg']), \n",
    "        kernel_initializer=initializer,\n",
    "        bias_initializer=bias_initializer))\n",
    "\n",
    "    optimizer = SGD(learning_rate=hparams['lr'], momentum=hparams['momentum'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[mean_euclidean_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed631de-ecc3-4798-a25b-5c4762fc6e53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NN-ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea59c9-c244-403d-aadf-e35d2df6df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration NN-ADAM\n",
    "best_config_adam = {\n",
    "    'lr': 0.00026, \n",
    "    'n_layers': 3, \n",
    "    'h_dim': 128, \n",
    "    'activation': 'tanh', \n",
    "    'reg': 0.001, \n",
    "    'beta_1': 0.87, \n",
    "    'beta_2': 0.96, \n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "def get_nn_adam_regressor(hparams: dict, in_dim: int):\n",
    "    \"\"\"Returns a NN with ADAM regressor.\n",
    "    \n",
    "    Args:\n",
    "        - hparams: a set of hyper-parameters\n",
    "        - in_dim: input dimension [1]\n",
    "    \"\"\"\n",
    "    \n",
    "    if hparams['activation'] == 'tanh':\n",
    "        initializer = GlorotUniform(seed=RANDOM_STATE) # Glorot (Xavier)\n",
    "        bias_initializer = Zeros()\n",
    "    elif hparams['activation'] == 'ReLU':\n",
    "        initializer = HeNormal(seed=RANDOM_STATE) # He (Kaiming)\n",
    "        bias_initializer = Constant(0.1)\n",
    "        \n",
    "    reg = l2(hparams['reg'])\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        hparams['h_dim'], \n",
    "        activation=hparams['activation'], \n",
    "        input_shape=(in_dim,), \n",
    "        kernel_regularizer=l2(hparams['reg']),\n",
    "        kernel_initializer=initializer,\n",
    "        bias_initializer=bias_initializer))\n",
    "\n",
    "\n",
    "    h_dim = hparams['h_dim']\n",
    "    for i in range(hparams['n_layers'] - 1):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                h_dim, \n",
    "                activation=hparams['activation'],\n",
    "                kernel_regularizer=l2(hparams['reg']),\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=bias_initializer))\n",
    "        h_dim //= 2\n",
    "\n",
    "\n",
    "    model.add(Dense(\n",
    "        3, \n",
    "        activation='linear', \n",
    "        kernel_regularizer=l2(hparams['reg']), \n",
    "        kernel_initializer=initializer,\n",
    "        bias_initializer=bias_initializer))\n",
    "    \n",
    "    optimizer = Adam(\n",
    "        learning_rate=hparams['lr'],\n",
    "        beta_1=hparams['beta_1'], \n",
    "        beta_2=hparams['beta_2'])\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[mean_euclidean_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e542cf-f608-425d-8a37-d29f3fa3fc65",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1399919-b014-4c1f-bfe4-eb00f41aa628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration SVR\n",
    "best_config_svr = {\n",
    "    'C': 2000,\n",
    "    'epsilon': 0.07,\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 'scale',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72361cae-ffb0-4b00-bda8-0f9a24321d5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Validation\n",
    "Here we leverage the $70-20$ training-validation split to perform model selection w.r.t. the ensemble and the single models. Based on the validation MEE we're going to decide which is our model/ensemble for submitting the blind test predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af16b62-4aa4-4acd-8470-f6a86fb9abc3",
   "metadata": {},
   "source": [
    "### NN-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c5678-4a77-4625-bd69-aaa7bcbb4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val NN-SGD\n",
    "model_nn_sgd = get_nn_sgd_regressor(best_config_sgd, x_train_cup.shape[1])\n",
    "solver = Solver(model_nn_sgd, x_train_cup_poly, y_train_cup, x_internal_test_cup_poly, y_internal_test_cup, target='val_mean_euclidean_error')\n",
    "solver.train(epochs=800, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c57ffc-d7b8-4400-b0c2-5b3ffe8d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sgd_train_preds = model_nn_sgd.predict(x_train_cup_poly)\n",
    "nn_sgd_val_preds = model_nn_sgd.predict(x_val_cup_poly)\n",
    "mee_sgd_train = float(mean_euclidean_error(y_train_cup, nn_sgd_train_preds))\n",
    "mee_sgd_val = float(mean_euclidean_error(y_val_cup, nn_sgd_val_preds))\n",
    "\n",
    "print(f\"TRAIN MEE: {mean_euclidean_error(y_train_cup, nn_sgd_train_preds)} - VAL MEE: {mean_euclidean_error(y_val_cup, nn_sgd_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16550e-0565-4283-b2f2-4af316edeb18",
   "metadata": {},
   "source": [
    "### NN-ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3c03c-a491-4d13-b3ea-70dc7fa931b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val NN-SGD\n",
    "model_nn_adam = get_nn_sgd_regressor(best_config_sgd, x_train_cup.shape[1])\n",
    "solver = Solver(model_nn_adam, x_train_cup_poly, y_train_cup, x_internal_test_cup_poly, y_internal_test_cup, target='val_mean_euclidean_error')\n",
    "solver.train(epochs=1500, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17dae7-5a62-4c13-ab33-d39487963b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_adam_train_preds = model_nn_adam.predict(x_train_cup_poly)\n",
    "nn_adam_val_preds = model_nn_adam.predict(x_val_cup_poly)\n",
    "mee_adam_train = float(mean_euclidean_error(y_train_cup, nn_adam_train_preds))\n",
    "mee_adam_val = float(mean_euclidean_error(y_val_cup, nn_adam_val_preds))\n",
    "\n",
    "print(f\"TRAIN MEE: {mean_euclidean_error(y_train_cup, nn_adam_train_preds)} - VAL MEE: {mean_euclidean_error(y_val_cup, nn_adam_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541b9a5-a50b-4d95-9f79-41112cdc70b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa458e3-36f5-4ae3-8d4a-e34929357879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVR\n",
    "multi_svr = MultiOutputRegressor(SVR(**best_config_svr))\n",
    "multi_svr.fit(x_train_cup_poly, y_train_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12795722-1331-4e85-8e7f-9d1d294fa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_train_preds = multi_svr.predict(x_train_cup_poly)\n",
    "svr_val_preds = multi_svr.predict(x_val_cup_poly)\n",
    "mee_svr_train = mee(y_train_cup, svr_train_preds)\n",
    "mee_svr_val = mee(y_val_cup, svr_val_preds)\n",
    "\n",
    "print(f\"TRAIN MEE: {mee(y_train_cup, svr_train_preds)} - VAL MEE: {mee(y_val_cup, svr_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfcbf7-f5d8-42f3-a3d3-e0d82e97ec40",
   "metadata": {},
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d2cc9-181e-4204-a859-5f2c8658adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic average\n",
    "ar_ensemble_val_preds = ensemble_arithmetic_average(svr_val_preds, nn_sgd_val_preds, nn_adam_val_preds)\n",
    "print(f\"ARITHMETIC AVERAGE VAL MEE: {mean_euclidean_error(y_val_cup, ar_ensemble_val_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ead7f-5108-435b-9dd5-dd335f5c817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average\n",
    "wa_ensemble_val_preds = ensemble_weighted_averange(\n",
    "    1, svr_val_preds, nn_sgd_val_preds, \n",
    "    nn_adam_val_preds, mee_svr_train, \n",
    "    mee_sgd_train, mee_adam_train\n",
    ")\n",
    "print(f\"WEIGHTED AVERAGE VAL MEE: {mean_euclidean_error(y_val_cup, wa_ensemble_val_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f356f-0bcd-4640-92d4-717bd32b450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Stacking. Valid estimator: LinearRegression, Ridge or Lasso\n",
    "#stacking_estimator = LinearRegression()\n",
    "#stacking_estimator = Lasso(alpha=0.02)\n",
    "stacking_estimator = Ridge(alpha=30)\n",
    "x_stack_cup_train = np.hstack((svr_train_preds, nn_sgd_train_preds, nn_adam_train_preds))\n",
    "x_stack_cup_val = np.hstack((svr_val_preds, nn_sgd_val_preds, nn_adam_val_preds))\n",
    "stacking_estimator.fit(x_stack_cup_train, y_train_cup)\n",
    "\n",
    "print(f\"RIDGE VAL MEE: {mean_euclidean_error(y_val_cup, stacking_estimator.predict(x_stack_cup_val))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77aaf2b-2fc8-4b61-bc7b-255e1cfa0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name = \"alpha\"\n",
    "param_range = np.linspace(0, 80, 100)\n",
    "\n",
    "ValidationCurveDisplay.from_estimator(estimator, \n",
    "                                      x_stack_cup_val, \n",
    "                                      y_val_cup, \n",
    "                                      param_name=param_name, \n",
    "                                      param_range=param_range,\n",
    "                                      cv=None,\n",
    "                                      scoring= make_scorer(mee, greater_is_better = False),\n",
    "                                      negate_score = True,\n",
    "                                      score_name=\"MEE\",\n",
    "                                      verbose=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ab2fa-3bf9-45b9-a136-d648e04aa10b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Internal test assessment\n",
    "Let's perform a re-training of our model on the entire training/validation set. In this way, we're able to leverage the entire training/validation data (early stopping is applied w.r.t. the train MEE). In order to obtain the ensembles' estimates on the (untouched) internal test we load the storeded predictions of the chosen models. Therefore, we're able to perform model assessment and estimate the ensembles' performance on the blind test. Additionally, by using the same predictions used for model assesment of the single models (instead of re-training them) we believe to ensure more meaningful results.\n",
    "\n",
    "Note that, in this phase, we don't use the internal test in any way (i.e., no training and no validation). We only estimate its errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d34b2f-3f84-4910-9ac0-ce30c38cf79b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0ed60-6df1-481a-b7dd-9112b8855be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cup_predictions(in_path: str):\n",
    "    \"\"\"Utility function to load blind test predictions of a model.\n",
    "    \n",
    "    Args:\n",
    "        - in_dir: the .csv file path\n",
    "    \"\"\"\n",
    "    preds = pd.read_csv(in_path, header=None, delimiter=',', skiprows=4)\n",
    "    preds.drop(columns=preds.columns[0], axis=1, inplace=True)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb9ab0-8e84-4d9f-8e21-beedb18e3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train predictions\n",
    "nn_sgd_trainval_preds = load_cup_predictions(root_path + '/results/NN-SGD/CUP/mean_5_train_preds_poly.csv')\n",
    "nn_adam_trainval_preds = load_cup_predictions(root_path + '/results/NN-ADAM/CUP/mean_5_train_preds_poly.csv')\n",
    "svr_trainval_preds = load_cup_predictions(root_path + '/results/SVM/CUP/train_preds_poly.csv')\n",
    "\n",
    "# Load internal test predictions\n",
    "nn_sgd_internal_test_preds = load_cup_predictions(root_path + '/results/NN-SGD/CUP/mean_5_internal_test_preds_poly.csv')\n",
    "nn_adam_internal_test_preds = load_cup_predictions(root_path + '/results/NN-ADAM/CUP/mean_5_internal_test_preds_poly.csv')\n",
    "svr_internal_test_preds = load_cup_predictions(root_path + '/results/SVM/CUP/internal_test_preds_poly.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d1c5-7f30-4c4f-9c95-a4fc59adc210",
   "metadata": {},
   "source": [
    "For the weighted average we also need to load, for each model, the MEE on the training/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b81b0-44b0-4dcd-945a-67c9f56d9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cup_train_mee(in_path: str):\n",
    "    \"\"\"Utility function to train MEE of a model.\n",
    "    \n",
    "    Args:\n",
    "        - in_dir: the .json file path\n",
    "    \"\"\"\n",
    "    with open(in_path) as inf:\n",
    "        data = json.load(inf)\n",
    "        try:\n",
    "            return data['train']['mean_mee']\n",
    "        except KeyError:\n",
    "            return data['train']['mee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2b383-5a0a-4b5a-8bf8-c87a215ba324",
   "metadata": {},
   "outputs": [],
   "source": [
    "mee_sgd_train = load_cup_train_mee(root_path + '/results/NN-SGD/CUP/mean_5_report_poly.json')\n",
    "mee_adam_train = load_cup_train_mee(root_path + '/results/NN-ADAM/CUP/mean_5_report_poly.json')\n",
    "mee_svr_train = load_cup_train_mee(root_path + '/results/SVM/CUP/report_poly.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55978ec9-9a54-411a-8fb5-1f0f44e0c330",
   "metadata": {},
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375bf2d2-5546-4c87-93a5-c6a66403bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic average\n",
    "ar_ensemble_train_preds = ensemble_arithmetic_average(svr_trainval_preds, nn_sgd_trainval_preds, nn_adam_trainval_preds)\n",
    "ar_ensemble_internal_test_preds = ensemble_arithmetic_average(svr_internal_test_preds, nn_sgd_internal_test_preds, nn_adam_internal_test_preds)\n",
    "\n",
    "print(f\"ARITHMETIC AVERAGE TRAIN MEE: {mean_euclidean_error(y_dev_cup_tmp, ar_ensemble_train_preds)}\")\n",
    "print(f\"ARITHMETIC AVERAGE INTERNAL TEST MEE: {mean_euclidean_error(y_internal_test_cup, ar_ensemble_internal_test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc86cf6-5853-498c-8599-5fce9c36c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average (weights w.r.t. lower train MEE)\n",
    "wa_ensemble_train_preds = ensemble_weighted_averange(\n",
    "    1, svr_trainval_preds, nn_sgd_trainval_preds, \n",
    "    nn_adam_trainval_preds, mee_svr_train, \n",
    "    mee_sgd_train, mee_adam_train\n",
    ")\n",
    "wa_ensemble_internal_test_preds = ensemble_weighted_averange(\n",
    "    1, svr_internal_test_preds, nn_sgd_internal_test_preds, \n",
    "    nn_adam_internal_test_preds, mee_svr_train, \n",
    "    mee_sgd_train, mee_adam_train\n",
    ")\n",
    "\n",
    "print(f\"WEIGHTED AVERAGE TRAIN MEE: {mean_euclidean_error(y_dev_cup_tmp, wa_ensemble_train_preds)}\")\n",
    "print(f\"WEIGHTED AVERAGE INTERNAL TEST MEE: {mean_euclidean_error(y_internal_test_cup, wa_ensemble_internal_test_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59a5e7-ac0f-45c3-bcc6-05076cf24de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Ridge\n",
    "stacking_estimator = Ridge(alpha=30)\n",
    "x_stack_cup_trainval = np.hstack((svr_trainval_preds, nn_sgd_trainval_preds, nn_adam_trainval_preds))\n",
    "x_stack_cup_internal_test = np.hstack((svr_internal_test_preds, nn_sgd_internal_test_preds, nn_adam_internal_test_preds))\n",
    "stacking_estimator.fit(x_stack_cup_trainval, y_dev_cup_tmp)\n",
    "\n",
    "print(f\"STACKING RIDGE TRAIN MEE: {mean_euclidean_error(y_dev_cup_tmp, stacking_estimator_train.predict(x_stack_cup_trainval))}\")\n",
    "print(f\"STACKING RIDGE INTERNAL TEST MEE: {mean_euclidean_error(y_internal_test_cup, stacking_estimator.predict(x_stack_cup_internal_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a646e09-f989-4f76-8893-4b88ab65ac29",
   "metadata": {},
   "source": [
    "# Final re-training\n",
    "Since the test error has already been estimated by leveraging the (untouched) internal test, we now perform a final re-training with all the development data. This does not violate the rules, since the internal test is not (and has never) been used for any model selection.\n",
    "\n",
    "Thus, we train on the entire development data, i.e. $90$ train/val + $10$ internal test. Early stopping is w.r.t. the MEE.\n",
    "\n",
    "The best performing model/ensemble - according to the hold-out validation previously performed - is the **Stacking Regressor (Ridge)**. This will be the our final model/ensemble to obtain the blind test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4c309-ea18-4591-972f-0997a8040407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply polynomial to the entire development set\n",
    "x_dev_cup = poly.transform(x_dev_cup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75c7af-070d-4e66-90b9-687375c11784",
   "metadata": {},
   "source": [
    "### Load blind-test predictions\n",
    "We load each model blind test prediction after their final re-training (i.e., the re-training on all development data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21faa053-8714-4a7d-b067-b8f5544c56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final train predictions\n",
    "nn_sgd_final_train_preds = load_cup_predictions(root_path + '/results/NN-SGD/CUP/final_train.csv')\n",
    "nn_adam_final_train_preds = load_cup_predictions(root_path + '/results/NN-ADAM/CUP/final_train.csv')\n",
    "svr_final_train_preds = load_cup_predictions(root_path + '/results/SVM/CUP/final_train.csv')\n",
    "\n",
    "# Load final blind test predictions\n",
    "nn_sgd_blind_test_preds = load_cup_predictions(root_path + '/results/NN-SGD/CUP/final_blind_test.csv')\n",
    "nn_adam_blind_test_preds = load_cup_predictions(root_path + '/results/NN-ADAM/CUP/final_blind_test.csv')\n",
    "svr_blind_test_preds = load_cup_predictions(root_path + '/results/SVM/CUP/final_blind_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1997536-2e15-41b1-85f5-a10dc5d19913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking with Ridge on blind test set\n",
    "stacking_estimator_final = Ridge(alpha=30)\n",
    "x_stack_cup_final_train = np.hstack((svr_final_train_preds, nn_sgd_final_train_preds, nn_adam_final_train_preds))\n",
    "x_stack_cup_blind_test = np.hstack((svr_blind_test_preds, nn_sgd_blind_test_preds, nn_adam_blind_test_preds))\n",
    "stacking_estimator_final.fit(x_stack_cup_final_train, y_dev_cup)\n",
    "\n",
    "stacking_final_blind_test_preds = stacking_estimator_final.predict(x_stack_cup_blind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81c7fe-a6f7-4ee2-b2ee-43a257f0b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final blind test mean predictions\n",
    "with open(results_dir + '/CUP/final_blind_test.csv', 'w') as outf:\n",
    "    # Team Info\n",
    "    outf.write(\"# Matteo Pinna, Leonardo Caridi, Marco Sanna\\n\")\n",
    "    outf.write(\"# ACD-TEAM\\n\")\n",
    "    outf.write(\"# ML-CUP23 v2\\n\")\n",
    "    outf.write(\"# 20/01/2024\\n\")\n",
    "\n",
    "    # Writing predictions\n",
    "    for i, pred in enumerate(stacking_final_blind_test_preds, 1):\n",
    "        outf.write(f\"{i},{','.join(map(str, pred))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
